# -*- coding: utf-8 -*-
"""Fine Tunned LLM for Text Summarization

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/118njYQVVY_RWmRXltyUueJSH2-kPDPG-
"""

!pip install datasets

!pip install evaluate

!pip install evaluate rouge-score

# Import necessary libraries
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from datasets import load_dataset
from sklearn.metrics import accuracy_score
import numpy as np

# Load the dataset (Use a smaller subset to prevent long runtimes)
dataset = load_dataset("cnn_dailymail", "3.0.0")  # Load only 1% of the dataset
val_dataset = load_dataset("cnn_dailymail", "3.0.0")

from datasets import load_dataset

# Load the dataset splits
dataset_dict = load_dataset("cnn_dailymail", "3.0.0")
train_dataset = dataset_dict["train"]
val_dataset = dataset_dict["validation"]

# Determine the size of the subset
train_size = min(2000, len(train_dataset))  # Limit to 1000 samples or total size
val_size = min(500, len(val_dataset))  # Limit to 500 samples or total size

# Slice the datasets to a smaller subset
train_dataset = train_dataset.select(range(train_size))
val_dataset = val_dataset.select(range(val_size))

# Load the pre-trained T5 model and tokenizer (use t5-tiny for faster training)
model_name = "google/t5-v1_1-small"  # Use a smaller model to reduce memory usage and speed up training
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

# Enable gradient checkpointing to save memory
model.gradient_checkpointing_enable()

# Tokenization and Preprocessing Function
def preprocess_function(examples):
    inputs = ["summarize: " + doc for doc in examples["article"]]
    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding="max_length")  # Reduce max_length
    labels = tokenizer(examples["highlights"], max_length=64, truncation=True, padding="max_length").input_ids
    model_inputs["labels"] = labels
    return model_inputs

# Apply tokenization function
def tokenize_dataset(dataset, batch_size=16):
    # Tokenize dataset with smaller batch size
    return dataset.map(
        preprocess_function,
        batched=True,
        batch_size=batch_size,  # Reduce batch size
        num_proc=2  # Reduce number of processes
    )

# Tokenize the datasets
tokenized_train = tokenize_dataset(train_dataset)
tokenized_val = tokenize_dataset(val_dataset)

# Use a smaller subset of the dataset to speed up training and avoid session crashes
small_train_dataset = tokenized_train.select(range(min(200, len(tokenized_train))))  # Select up to 1000 samples for training
small_val_dataset = tokenized_val.select(range(min(50, len(tokenized_val))))  # Select up to 500 samples for validation

# Optimized Training Arguments
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy='epoch',  # Evaluate at the end of each epoch
    save_strategy="epoch",  # Save at the end of each epoch
    learning_rate=5e-5,  # Slightly higher learning rate for faster convergence
    per_device_train_batch_size=2,  # Smaller batch size to prevent memory issues
    gradient_accumulation_steps=2,  # Accumulate gradients to simulate larger batch size
    num_train_epochs=1,  # Train for 1 epoch initially to test
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=500,  # Log less frequently to save time
    fp16=True if torch.cuda.is_available() else False,  # Mixed precision for GPU
    save_total_limit=2,  # Limit saved checkpoints
)

!pip install --upgrade transformers

# Metric computation
def compute_metrics(pred):
    predictions = pred.predictions
    labels = pred.label_ids

    if isinstance(predictions, tuple):
        predictions = predictions[0]
    if isinstance(labels, tuple):
        labels = labels[0]

    if isinstance(predictions, list):
        predictions = np.array(predictions)
    if isinstance(labels, list):
        labels = np.array(labels)

    if predictions.ndim > 2:
        predictions = np.squeeze(predictions)
    if labels.ndim > 2:
        labels = np.squeeze(labels)

    if predictions.dtype == np.float32:
        predictions = np.argmax(predictions, axis=-1)

    min_length = min(len(predictions), len(labels))
    predictions = predictions[:min_length]
    labels = labels[:min_length]

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    metric = load("rouge")
    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
    result = {key: value * 100 for key, value in result.items()}

    return result

# Evaluate the model BEFORE fine-tuning
trainer_pretrained = Trainer(
    model=model,
    args=training_args,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer
)

print("Evaluating the pre-trained model (before fine-tuning)...")
pretrained_results = trainer_pretrained.evaluate()
print("Pre-trained model performance (before fine-tuning):", pretrained_results)

# Fine-tune the model
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

print("Fine-tuning the model...")
trainer.train()

# Evaluate the model AFTER fine-tuning
print("Evaluating the fine-tuned model...")
fine_tuned_results = trainer.evaluate()
print("Fine-tuned model performance:", fine_tuned_results)

# Extract and compare ROUGE scores
pretrained_rouge = pretrained_results.get('eval_rouge1', 0)
fine_tuned_rouge = fine_tuned_results.get('eval_rouge1', 0)

print(f"ROUGE-1 score before fine-tuning: {pretrained_rouge:.2f}%")
print(f"ROUGE-1 score after fine-tuning: {fine_tuned_rouge:.2f}%")

# Evaluate the pre-trained model on the smaller subset
print("Evaluating the pre-trained model (before fine-tuning)...")
with torch.no_grad():  # Disable gradient calculations to save memory
    pretrained_results = trainer_pretrained.evaluate(eval_dataset=small_val_dataset)
print("Pre-trained model performance (before fine-tuning):", pretrained_results)

# Evaluate and get results
pretrained_results = trainer_pretrained.evaluate()
pretrained_accuracy = pretrained_results["eval_accuracy"]
print(pretrained_accuracy)

# Fine-tune the model
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,  # Use smaller dataset to prevent crashes
    eval_dataset=small_val_dataset,  # Use smaller validation dataset
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# Fine-tune the model
print("Fine-tuning the model...")
trainer.train()

# Extract and compare ROUGE scores
pretrained_rouge = pretrained_results.get('eval_rouge1', 0)
fine_tuned_rouge = fine_tuned_results.get('eval_rouge1', 0)

print(f"ROUGE-1 score before fine-tuning: {pretrained_rouge:.2f}%")
print(f"ROUGE-1 score after fine-tuning: {fine_tuned_rouge:.2f}%")

